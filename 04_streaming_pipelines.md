# Streaming data pipelines

Streaming Extract, Transform, Load (ETL) is a foundational use case for processing continuous data streams. It allows you to ingest, transform, and load data in real-time or near real-time, making it immediately available for analysis or further processing. Streaming ETL can be categorized into two main types based on the complexity of transformations.

Use for: Real-time analytics and dashboarding for events that are already in a structured or semi-structured format. This data primarily needs to be loaded into BigQuery for immediate querying, often with minimal transformation required.

How it happens: Raw data is collected with Pub/Sub (or Managed Service for Apache Kafka), minimal transformations are applied with Dataflow, and then queried using SQL in BigQuery.

Examples: Website clickstream data, IoT sensor readings, or application logs that are relatively clean and consistent in their format.

Complex Streaming ETL

Use for: Scenarios where incoming data needs significant cleaning, joining with other datasets, aggregation, or schema restructuring before it's suitable for analytical queries. This requires more robust processing capabilities.

How it happens: Transformations occur before the data is loaded into BigQuery.

Examples: Calculating real-time Key Performance Indicators (KPIs), or preparing raw game event data for detailed reporting that requires a specific, transformed schema.

Streaming AI/ML

The next use case you'll learn about is streaming AI/ML. This is about applying machine learning models directly to incoming data streams to generate immediate insights or actions. You can achieve this by performing common preprocessing methods (like embedding generation or creating n-grams) on incoming data, then applying an ML model for real-time inference.

Key aspects of streaming AI/ML

Use for: Applying ML models to data as it arrives to make instant decisions.

How it happens: Perform preprocessing tasks like generating embeddings and bucketizing data, enrich events with a feature store, then call a model for inference.

Examples: Real-time fraud detection, personalized recommendations, anomaly detection in operational data, and real-time risk assessment

## Streaming applications

The use case of streaming applications leverages continuously updated data to build real-time, low-latency applications that require immediate access to information. These applications react instantly to streaming events, providing dynamic user experiences or backend services.

Key aspects of streaming applications

Use for: Data with low latency requirements that must be processed and made available for applications with minimal delay.

How it happens: Applications are powered by constantly refreshing data streams.

Examples: Updating a user's real-time spending limit based on new transactions, tracking game scores in real-time for live leaderboards during an e-sports tournament, or powering a personalized news feed that updates as relevant articles are published.

### Reverse ETL

The final streaming use you'll learn about here is reverse ETL. Reverse ETL is a growing use case that involves activating data from your data warehouse or analytical system by sending processed insights back to operational systems or external applications. This closes the loop, putting analytical findings directly into the hands of operational teams or automated systems.

Key aspects of reverse ETL

Use for: Leveraging valuable insights generated in BigQuery for operational uses.

How it happens: Refined data is distributed to platforms like marketing automation tools, CRM (Customer Relationship Management) systems, or other operational databases.

Examples: Sending customer segments defined in BigQuery to a marketing automation platform via Pub/Sub, updating user profiles in an operational database like Spanner or Bigtable based on analytical findings, or pushing transformed and aggregated data back into BigQuery for further analysis.

## Architectural considerations for Pub/Sub and Managed Service for Apache Kafka

To begin building your pipeline, the focus is on the ingestion and message broker components. These will lay the foundation for all downstream processing, analytics, storage, and machine learning so it's important the tool you choose is resilient and robust. This is where ingestion services like Pub/Sub and Managed Service for Apache Kafka come into play.

In a streaming data pipeline, data is in motion and constantly generated by sources like web clicks, financial transactions, and IoT devices. For Cymbal Gaming's Galactic Grand Prix use case, the data streaming in is in-game telemetry data, live broadcast and audience data, match outcomes, and tournament/market data.

Choose your streaming champion

One of the main reasons engineers choose Pub/Sub is because it's a fully managed, serverless messaging service where Google handles all of your underlying infrastructure, scaling, and maintenance, and it works seamlessly with other Google Cloud products. They choose Managed Service for Apache Kafka, on the other hand, because it is compatible with their existing systems, skills, or requirements (like Spark or KRaft).

Key Points

Pub/Sub

Managed Service for Apache Kafka

Managed vs. serverless

Pub/Sub is a serverless, "no-ops" service.

You still configure cluster resources like vCPUs and memory, but Google handles the difficult parts like automatic broker sizing, cluster creation, storage management, and rebalancing to save time and reduce costs.

Scalability

Scales automatically without limits or manual intervention.

You adjust the vCPU count and memory, and the service automatically provisions or scales brokers to meet demand.

Message retention

Primarily a message buffer with a default retention of 7 days (extendable to 31 days) and a 'seek' feature is used to replay messages.

Designed as a persistent log and can retain messages for long periods of time (even forever) allowing consumers to replay data from any offset.

Ordering and deduplication

Provides at-least-once delivery by default but offers exactly-once delivery which you can enable on a subscription. It offers ordering keys to maintain the order of messages with the same key.

Guarantees message order within a partition and can be configured for exactly-once processing.

Ecosystem integration

Offers seamless, native integration with other Google Cloud services like Dataflow, Cloud Storage, Cloud Functions, and BigQuery.

Provides portability with the open-source Kafka API and a vast ecosystem of connectors, making it ideal for multi-cloud or hybrid strategies. It includes managed Kafka Connect for easier integration with services like BigQuery and Cloud Storage.


Understanding integration points and use cases

While Pub/Sub and Kafka both handle real-time data streaming, they excel in different scenarios.

•
Pub/Sub as the ingestion layer: For new cloud-native applications on Google Cloud, Pub/Sub is the recommended choice for ingesting real-time data because it is fully managed and tightly integrated with Google Cloud products.

In the Galactic Grand Prix, Pub/Sub is perfect for the initial, rapid ingestion of raw gameplay events for real-time dashboards.

•
Managed service for Apache Kafka for portability and existing workloads: This service is the ideal choice if you need to migrate existing Kafka applications to Google Cloud with minimal code changes, require the specific features of the Kafka ecosystem, or want to maintain a consistent API across hybrid or multi-cloud environments.

For the Galactic Grand Prix, if existing internal scripts were built on the Kafka API they could be pointed to the managed service, preserving years of investment while offloading infrastructure management to Google.

•
Hybrid architectures with Kafka Connect: You can integrate Pub/Sub and Managed Service for Apache Kafka using the managed Kafka Connect feature. There are two types of connectors:

Source Connector: Pull messages from Pub/Sub topics and publish them to Kafka topics.

For example during the Galactic Grand Prix a Pub/Sub source Connector could pull real-time player chat messages from a Pub/Sub topic where they are initially ingested for broader analysis. These messages are then pushed to a Kafka topic, allowing a legacy, on-premises sentiment analysis engine (built years ago on Kafka Streams) to process them for immediate toxicity detection and moderation.

Sink Connector: Consume messages from Kafka topics and publish them to Pub/Sub topics for consumption by other Google Cloud services.

For example, a Pub/Sub Sink Connector could consume aggregated, low-latency match statistics (like lap times or percentage of track completions) from a Kafka topic. This data might be generated by specialized Kafka-based stream processing applications on a hybrid cloud setup. The connector then publishes these statistics to a Pub/Sub topic, enabling various Google Cloud services like BigQuery (for historical analysis) or Dataflow (for further transformation before display on real-time dashboards) to easily consume them.

Pub/Sub core concepts

A Pub/Sub system allows a Publisher (like the game servers in the Galactic Grand Prix) to send messages (game events) to a designated Topic ("game events"). Users (such as our Dataflow jobs for ETL or streaming AI/ML) first create a Subscription (ie- real_time_etl_sub) that is a unique queue linked to that topic, to receive these messages. Pub/Sub handles the message delivery based on the subscription type:

•
For Pull or Push subscriptions, subscriber applications must actively consume the data.

•
For BigQuery or Cloud Storage subscriptions, Pub/Sub automatically exports the messages to the specified destination. This process doesn't require an active subscriber application to handle the data transfer.

Pull subscription
This is where the subscriber application actively requests (pulls) messages from the Pub/Sub service when it is ready to process them. This model provides the subscriber with more control over the message consumption rate.

Push subscription
This service automatically sends (pushes) messages to a publicly accessible HTTP endpoint (a webhook) you specify. This is ideal for real-time applications and serverless architectures like Cloud Functions, where the subscriber is triggered by the arrival of a new message.

BigQuery subscription
A specialized subscription that automatically writes messages from a topic directly into a BigQuery table. This is a simple and efficient way to ingest streaming data for analytics without needing to run a separate processing job.

Cloud storage subscription
This is a specialized subscription that automatically writes messages from a topic into a Cloud Storage bucket as files.

**One of the most valuable benefits of Pub/Sub is its ability to decouple services.**

Pub/Sub supports flexible message distribution patterns which are directly and fundamentally connected to its ability to decouple services. The core of this relationship lies in how Pub/Sub acts like an intermediary, allowing components to communicate without needing to know anything about each other which is crucial for various architectural needs.

Below you'll explore three examples of flexible messaging patterns within Pub/Sub.

- Straight through: The simplest pattern. Imagine a pizza shop with one phone line. When a customer calls, one person can take the order.

Publisher: The customer calling in the order.
Topic: The single phone line.
Subscriber: The single employee who takes the order.

- Fan out: Now, imagine the pizza shop has a big screen in the kitchen. When an order comes in, the same order is displayed to everyone on the kitchen staff.

Publisher: A single employee who enters a new order into the system.
Topic: The order entry system.
Subscribers: The pizza chef, the drink station attendant, and the delivery driver all see the same 
order on their respective screens.

Fan In: Imagine that pizza shop now has orders that come from many places: a phone app, a website, and a kiosk inside the store. No matter where the order comes from, they all get sent to a single computer screen for the kitchen staff to see and fulfill.

Publishers: The app, the website, and the kiosk, all sending new orders.
Topic: The order system that receives incoming orders.
Subscriber: The single computer screen in the kitchen.

Choosing between the push and pull delivery models provides the fundamental mechanism for how your subscribers receive data. But in any real-world data pipeline, receiving a message is only half the battle. This next section covers the features that elevate Pub/Sub from a simple message queue to a managed, production-grade messaging backbone.

As a data engineer, you know that things go wrong. Messages can arrive malformed, downstream systems can fail, or specific events may need to be processed in strict order. These advanced capabilities for things like direct BigQuery integration, complex ETL, import topics, and single message transforms are the built-in tools that solve these problems, saving you from having to build complex workarounds in your own applications.

## Pub/Sub integrations and advanced features: Data integration

Direct BigQuery integration
Stream Pub/Sub messages directly into BigQuery tables using BigQuery subscriptions, including into BigLake tables for Apache Iceberg.

Dataflow for complex ETL
Dataflow is essential as an intermediary for complex transformations, validation, enrichment, or exactly-once processing between Pub/Sub and BigQuery.

Cloud functions
Implement workflow automation.

BigQuery Sharing (Analytics Hub)
Pub/Sub topics can be shared via BigQuery sharing.

Single message transforms (SMT)
SMTs enable lightweight modifications to message data and attributes directly within Pub/Sub. One example is a JavaScript user-defined function (UDFs) for lightweight, in-stream transformations.

Import topics
This capability makes ingesting data in a managed way seamless from cross-cloud sources like Cloud Storage, AWS MSK, AWS Kinesis, Confluent Cloud, and Azure Event Hub.

## Pub/Sub integrations and advanced features: Delivery and reliability

Push vs. pull
Subscribers can either actively pull messages or receive them via push to an HTTP/S endpoint.

Exactly-once delivery
Pub/Sub provides exactly-once delivery for pull subscriptions within a single region by using a persistent deduplication layer to ensure messages are not redelivered after a successful acknowledgment.

Message filtering
Subscriptions can be configured to deliver only messages matching specific attributes.

Message ordering
Offers ordering within keys, though it may slightly increase latency.

Dead-letter topics (DLTs)
Reroutes messages that fail processing, preventing pipeline blockage.

While Pub/Sub is an ideal starting point for new cloud-native applications, many organizations migrate to Google Cloud with existing workloads in an open-source ecosystem.

Managed Service for Apache Kafka core concepts

In Kafka, a Producer ("like some of our legacy game systems) publishes records (game events) to a designated Topic ("legacy_game_data"). These topics are divided into partitions, which are ordered, immutable sequences of records. Consumers (such as our Dataflow ML pipelines) then subscribe to one or more topics and read messages from them. Consumers typically belong to a Consumer Group, where each message from a topic's partitions is delivered to only one consumer within that group, enabling parallel processing and fault tolerance. Consumers manage their own offset (their position in the log for each partition), allowing them to control which messages they read and to re-read past messages if needed.

Consumers can receive messages via one main delivery method: the pull-based method. This means that a consumer client actively requests or "pulls" messages from a Kafka broker, rather than the broker "pushing" messages to the consumer. Here, the consumer is in control of when and how many messages it consumes, which allows for flexible and efficient consumption rates.

## AI and ML integration

Both Pub/Sub and Managed Service for Apache Kafka are crucial for building modern, event-driven ML systems by decoupling data ingestion from model processing. Watch the videos below to explore how these messaging systems integrate with Vertex AI to build intelligent, event-driven applications.

## Dataflow: The processing powerhouse

At this point in your streaming data pipeline, you have established an ingestion layer with Pub/Sub that reliably receives every lap completed, gear changed, and collision/incident taken from the Galactic Grand Prix. You have also learned about Managed Service for Apache Kafka and how these two services can work together or independently.

While these tools serve as your high-volume entry point for raw data--complex transformations, aggregations, and sophisticated analytics often require a powerful processing engine to prepare it for the final destination. This is where Dataflow becomes your powerhouse.

Dataflow provides a robust and scalable environment for your data pipelines written with the Apache Beam SDK, and manages the entire infrastructure layer so that you can focus on your business logic. Let's take a look at how Dataflow manages your data processing overhead, using the Galactic Grand Prix at times to bring this use case to life.

Complex data arrives from Pub/Sub

A successful Galactic Grand Prix requires more than tracking who wins each race. It also needs to capture JSON payloads from various in-game sensor data sources, streams of fan engagements, and records of the in-game economy. Each of these data types has a different structure and purpose, presenting a challenge for a single pipeline. Below are three components that work together inside the processing portion of the pipeline.

JSON payloads (in-game sensor data)
What it is: These are raw, often unstructured events from sources like player interactions. For example, every player's console or PC is a sensor that generates data in the form of JSON objects. These get published to a Pub/Sub topic, which carry that raw user interaction data into Dataflow.

Data structure: Each message is a key-value payload, for example { "event_id": "a1-b2", "player_id": "p-789", "event_timestamp": "2024-09-15T14:32:15.123Z", "event_type": "SPELL_CAST", "details": {"spell_id": "s-fireball", "target_id": "p-456"} }.

Dataflow processing: Your first step in Dataflow is a "parallel do" or ParDo transform to parse this JSON string, extract the event_timestamp to use for event-time processing, and transform it into a Beam Row or a custom Python object for type-safe access downstream.

User interaction schemas and dataflow processing
What it is: While the JSON payload is the actual data, a user interaction schema is the blueprint for what that data should look like. It provides the structure and data types for the JSON objects, ensuring data quality and consistency.

Dataflow processing: Since Dataflow subscribes to the Pub/Sub topic, it ingests the raw JSON strings. The first step in Dataflow is to deserialize the data. A ParDo transform is used to parse the JSON string, convert it back into a structured object, and extract fields like the event_timestamp for event-time processing. This process converts the raw data into a structured PCollection (like a Beam Row or a custom Python object) that can be efficiently processed downstream.

Other serialization options: For high-volume streaming, a serialization framework like Protobuf or Avro can be used. These frameworks use a pre-defined schema to serialize data into a highly compact, binary format before it's sent to a Pub/Sub or Managed Service for Apache Kafka topic. When the data reaches Dataflow, a custom decoder or a built-in Beam SchemaTransform deserializes the binary records.

Structured transaction events (in-game economy)
What it is: These represent "clean" data that often needs to be enriched, rather than parsed. By joining this type of data with other data sources, you can add context and value. This data often comes from a separate source like a game's internal ledger, for instance, in-game purchases from the Galactic Grand Prix's transaction ledger.

Data structure: This is highly structured data, often { "transaction_id": "t-abc", "player_id": "p-789", "item_id": "i-sword", "gold_spent": 150 }.

Dataflow processing: The primary task here is enrichment. This involves joining a field like "item_id" with an input from a service like Cloud Storage to add more detailed metadata (like an item category or power level) to the stream. This process enhances the data's value for downstream analysis.

Dataflow processing strategies

- Parallel processing: What's Happening: This method is used to process high volumes of data. Instead of your @beam.DoFn being invoked once for every single game event, the Dataflow runner internally buffers elements into an in-memory collection called a "bundle." Your function is then applied to the entire bundle. For the data, this means that instead of one (key, value) pair, your code might receive a list or iterator of pairs. This dramatically reduces the per-element invocation overhead and CPU context switching, boosting throughput for tasks like simple data-type conversions or filtering, where the per-element logic is not complex.

- Windows and triggers: What's Happening: This method is for handling out-of-order data. It is a multi-stage process for a single specific window.

Initial Firing: The trigger fires (for example, based on EVENT TIMESTAMP), and your CombineFn (like a Mean or Sum) processes all data currently held in the window's state. The result is emitted to the output PCollection.

Late Data Arrives: A late event for that same window arrives.

Accumulation mode (discarding): In this mode, once a trigger fires and a result is emitted, the processed data for that window is discarded. The next time the trigger fires, it will only process and emit a result based on any new elements that have arrived since the last firing. This is used for use cases that require independent, periodic snapshots of data, not a continually refined result.

Accumulation mode (accumulation): Because the trigger is configured for late firings and the accumulation mode is set to ACCUMULATING, the CombineFn is invoked again. Crucially, it merges the existing state (the previous result) with the new late data. It then emits a new, updated result to the output PCollection. Downstream, a sink like BigQuery can be configured to use this new result to UPDATE or INSERT the row, ensuring the final analytics are correct.

- Data sinks: What's Happening: A sink is the final PTransform that writes your processed PCollection to an external system.

To BigQuery: The WriteToBigQuery transform serializes each element of your PCollection into a protobuf format and uses the high-throughput BigQuery Storage Write API to stream the records into a BigQuery table. This bypasses the slower, quota-limited legacy insertion methods.

To Bigtable: The WriteToBigtable transform converts each element into a DirectRow mutation object. For each element, you specify the row_key (e.g., the player_id), column family, column qualifier, and value. Dataflow then efficiently batches these mutations and writes them to the correct Bigtable tablet servers, optimized for low-latency writes.

Understanding these core processes—how Dataflow handles data in bundles, refines results over time, and connects to sinks—is the foundation for appreciating how it solves the most difficult real-world streaming challenges. Now, let's see how these concepts are applied to conquer the specific, high-stakes problems faced during the Galactic Grand Prix.

### Using Dataflow to overcome challenges of the Galactic Grand Prix

Latency and ordering
Challenge: Player events from servers in different continents arrive out of order.

Dataflow's solution: Dataflow processes data in event time, not arrival time. Its watermark system acts as a shared "game clock," allowing it to understand the true sequence of events.

Scalability
Challenge: A surprise comeback in a key match causes viewership and event traffic to surge 10x.

Dataflow's solution: Dataflow is serverless and autoscales automatically, spinning up and down workers as needed based on the throughput and complexity of your pipelines.

Fault tolerance
Challenge: A worker node processing a critical anti-cheat analysis pipeline fails.

Dataflow's solution: Dataflow provides the streaming engine backend, which provides a persistent state to make sure data is not lost or duplicated. It also redirects tasks to replacement workers.

Data quality and completeness
Challenge: In the final lap, two problems arise: a car's sensor sends a corrupted speed reading, and the winner's "finish line" event is delayed by network lag. You can't let a bad record crash the live dashboard, and you can't declare the wrong winner.

Dataflow's solution: Dataflow handles both. For quality, a dead-letter pattern in your Beam code instantly isolates the bad sensor data for debugging while the main pipeline continues uninterrupted. For completeness, watermarks, triggers, and allowed lateness act as your photo-finish toolkit. A provisional lap time is shown instantly. Then, withAllowedLateness keeps the time window open just long enough for the delayed data to arrive, allowing a late trigger to fire and push the official, corrected result to the leaderboard ensuring both speed and accuracy.

Predictive analytics and fair play
Challenge: A new, subtle cheat is giving some players an unfair advantage. It’s impossible to spot manually in the millions of events per second. You need to detect and act on this behavior in real-time.
Dataflow's solution: Dataflow ML can deploy a pre-trained anomaly detection model directly into the pipeline.

Real-time Inference: After parsing the raw event data, you add a RunInference transform. This transform feeds each player's stream of actions (e.g., aiming speed, resource collection rates) into a PyTorch or TensorFlow model trained to distinguish between human and bot-like behavior.

Immediate Action: If the model flags a player's actions as highly anomalous, the pipeline can automatically send that player's data to a separate BigQuery table for moderator review and trigger a real-time alert. This turns your data pipeline into an active defense system.

Now that you understand some of the challenges, we need to address them.

Learn how to address these challenges with windowing, mapping, grouping, and turnkey transformations.

- Fixed windows: Use case: To show "Total Gold Earned per Team" in a simple, updating bar chart on-screen, you'd use a fixed window. It gives a clear, periodic summary.

- Sliding windows (Hopping): Use case: To show a player's "Actions Per Minute" (APM) as a rolling average over the last 3 minutes, updated every 15 seconds. This gives casters a smooth, up-to-date metric on player intensity.

- Session windows: Use case: To automatically identify when a player has disconnected or gone AFK (Away From Keyboard). A session window groups all of a player's actions until a gap of inactivity (for instance 2 minutes) occurs, which can then trigger an alert.

At this point we've constructed a complete, robust Dataflow processing pipeline. It's no longer just a concept but a tangible asset designed to turn raw race data into consumable, meaningful insights. Below is an architecture of this robust Dataflow processing pipeline, designed to transform raw race telemetry data into actionable insights for both live dashboards and post-race analysis.

At this point, you know how Dataflow handles the challenges of the Galactic Grand Prix, turning a chaotic flood of events into clean, reliable data. But what if you could use that data to predict the future? What if you could identify a player about to churn, or flag a cheater before they ruin a match?

This is where Dataflow ML extends the processing powerhouse. By integrating machine learning models directly into your pipelines, you can evolve from historical analysis to real-time predictive intelligence. The primary tools for this are transforms. These allow you to run pre-trained ML models as seamlessly as any other step in your Dataflow pipeline.

### Advanced application: Building a real-time intelligent agent with Dataflow ML

So far, you've used Dataflow as a powerful engine for real-time analytics in the Galactic Grand Prix use case. You calculated lap times, aggregated stats, and ensured data accuracy. But what happens when you move beyond analytics and into the realm of real-time AI and ML? Dataflow isn't just for transforming data, it's also designed to be the backbone of Streaming AI/ML applications. To showcase this, let's pivot from the Grand Prix example to a new use case: a real-time "Intelligent Sales Agent" for a retail business.

## BigQuery: The analytical engine

At this point, the streaming data pipeline has taken shape. You've leveraged the ingestion layer of Pub/Sub (and learned about Managed Service for Apache Kafka) to capture a massive stream of data from the Galactic Grand Prix. Then you used the powerful processing engine of Dataflow to cleanse, enrich, and transform that data making it usable. But what happens after the data is processed? Where does it go so that it can be analyzed and used to generate insights? And what if you want to leverage real-time stream processing by simply running a SQL statement?

BigQuery is the right tool when you need to answer questions using a large dataset, even if the query pattern is unknown.

BigQuery is a serverless, highly scalable, and cost-effective analytical data platform that can query petabytes of prepared streaming data in a matter of seconds. Because BigQuery is a SQL columnar storage system, it's optimized for analytical queries making it perfect for answering business-critical questions, in our case, questions like:

Which drivers have the most consistent lap times across different races?

What is the correlation between gear changes and driver incidents?

How does the weather affect race outcomes?

In this section we'll perform deep analytics in order to bring Cymbal Gaming's Galactic Grand Prix to life as we move through the various ways to leverage BigQuery including:

- Data ingestion and streaming: You'll explore various data ingestion methods including how to configure Pub/Sub to BigQuery Streaming. This will allow us to move our real-time race data directly into BigQuery tables. You'll also dive into architecting robust BigQuery streaming pipelines using common design patterns.

- Continuous queries and analytics: You'll go over how to use BigQuery's continuous queries to perform real-time analysis on incoming data. This enables you to get immediate insights as the data streams in, all from the simplicity of a SQL statement.

- ETL and reverse ETL: You'll see how to use BigQuery for both ETL and Reverse ETL processes including transforming our data for final storage, then exporting it to other tools for business intelligence and operational uses.

### BigQuery's various data ingestion methods

Batch vs. streaming ingestion

First, let's take a look at when to use batch vs. streaming ingestion methods. This decision is determined by your data and whether or not it must be processed on a set schedule in large chunks, or continuously in real-time as it arrives.

Ingestion type

When to use

How it works

Key products

Limitations + cost

Batch ingest

Large, bounded datasets where data freshness is not the primary concern. Think hourly loads of historical data, logs, or data from other systems.

Data is collected and loaded into BigQuery in discrete, large chunks. This is the most cost-effective method for high-volume data that isn't needed in real-time.

BigQuery load jobs, BigQuery data transfer service (DTS), Spark/Hadoop connectors

While highly scalable and cheap for bulk data, it introduces latency. You're operating on data that is hours or even a day old. Load jobs are free, but you pay for storing data and compute for any transformations

Streaming ingest

Essential for real-time analytics, anomaly detection, and operational dashboards where immediate data availability is critical. Think IoT sensor data, application clickstreams, or financial transactions.

Data is written to BigQuery row-by-row or in small micro batches, making it available for query within seconds.

BigQuery Storage Write API, Pub/Sub (with a BigQuery subscription), Dataflow

Streaming comes at a higher cost per GiB compared to batch loading as you are paying for an always-on, low-latency ingestion mechanism. The legacy tabledata.insertALL API has limitations on row size and throughput, making the Storage Write API the preferred choice for new development.


### ETL vs ELT

Your choice of ingestion tool is directly influenced by when and where you plan to transform your data, which brings us to a crucial refresher on ETL vs. ELT.

•
Traditional ETL: You extract data from a source, transform it in a separate processing engine (like Dataflow, a dedicated Spark cluster, or a traditional ETL tool), and finally load the clean, structured data into the warehouse.

•
Modern ELT with BigQuery: You extract data from a source and immediately load it into a staging table in BigQuery. BigQuery's immense, low-cost compute power is then used to perform the transformations in place using SQL.

BigQuery's architecture and power fundamentally shift the paradigm towards ELT because it:

1
Reduces pipeline complexity by removing the need for a separate transformation engine for many use cases.

2
Leverages BigQuery’s serverless, massively parallel processing engine for transformations.

3
Preserves raw data, allowing you to re-run transformations or perform new types of analysis without having to re-ingest it.

So if your goal is to load raw data into BigQuery first, the next logical question is, "How do I do that efficiently for streaming data?" And the answer is the BigQuery Storage Write API.

### The BigQuery Storage Write API

### BigQuery Data Transfer Service (DTS)

While the Storage Write API is ideal for real-time ingestion, many data integration tasks are based on a schedule. For scenarios where you need to automatically and reliably pull data from other applications or cloud storage, we use the BigQuery Data Transfer Service (DTS).

Scheduled loads: BigQuery Data Transfer Service (DTS)

Not all data is streaming data. Many systems operate on a batch or scheduled basis. For this, BigQuery provides the DTS.

DTS is not a streaming service.

DTS is a fully managed, automated data loading service for scheduled transfers which is incredibly useful for pull data from other Google applications like Ads and YouTube, or from cloud storage services like Amazon S3 on a recurring schedule.

DTS is a "set it and forget it" way to ingest data from supported third-party sources.

DTS solves the problem of automated batch loading, however, it operates on schedules and moves large chunks of data. For use cases that demand a near-instant, row-by-row mirror of a production database in BigQuery, we need a different technology altogether.

- Change Data Capture (CDC)

...

- Complex transformations: Dataflow and Continuous queries

While BigQuery's built-in CDC capabilities are ideal for keeping your tables current with a source system's changes, handling more complex transformations and stateful processing requires a dedicated processing engine like Dataflow.

Watch the last data ingestion video below to learn how Dataflow can be used to perform complex transformations, aggregations, and enrichment on streaming data before it is loaded into BigQuery for analysis.

Now that you've learned how to get your streaming data into BigQuery, it's time to learn how to process and act on that data in real-time.

In this next section, you'll explore the concepts of continuous queries and reverse ETL, and how they can be used to power a new generation of event-driven applications.

Understanding BigQuery Continuous Queries

Earlier in this section you read that your choice of ingestion tool is directly influenced by when and where you plan to transform your data, and that traditional ETL has long been used to transform and prepare data before it's loaded into a warehouse.

BigQuery continuous queries are a modern evolution of the classic ETL process, powerful enough for real-time transformations. They are SQL statements that execute continuously, processing data as it arrives in your BigQuery tables in real-time. This allows you to perform time-sensitive tasks, such as generating real-time insights, applying machine learning models, and replicating data to other platforms. It’s like having an analyst who is always on, constantly monitoring your data streams and triggering actions the moment something noteworthy occurs.

The following terms describe how continuous queries work and what they're used for.

- Continuous execution
Queries run indefinitely to process new data.

-SQL-based
Uses the familiar language of SQL to define real-time data transformations and analysis.

- Event-driven
Triggers actions based on incoming data, turning BigQuery into an event-driven processing engine.

- Simplified real-time pipelines
Eliminates the need for additional technologies and specialized programming skills.

- Real-time AI use cases
Integrates with Vertex AI and Gemini to enable real-time AI-powered applications.

- Streamlined reverse ETL
Easily sends the results of a continuous query to other systems like Pub/Sub and Bigtable.

- Scalability and performance
Backed by BigQuery's serverless infrastructure to handle massive data volumes with high throughput and low latency.

#### Dataflow
Dataflow, which runs Apache Beam pipelines, acts as a sophisticated intermediary.

It pulls data from a source like Pub/Sub, performs your complex transformations in a highly scalable and resilient manner, and then writes the processed, analysis-ready data into BigQuery.

Gives flexibility to write to multiple BigQuery tables dynamically, or write to other systems.

#### Bigquery continuous queries

These allow you to define a long running SQL query that automatically processes new data as it arrives in a source table, with the results written to a destination BigQuery table, Pub/Sub topic, Bigtable table, or Spanner table.

Think of them as a lightweight, SQL-native ETL tool within BigQuery itself.

### Reverse ETL

In addition to ETL, a key benefit to streamlining data is reverse ETL. In this section you'll explore exactly what that means and how you can use continuous queries to send real-time insights from BigQuery back into your operational applications, which include Bigtable as you'll see later on.

Reverse ETL is the process of moving data from a storage system, like BigQuery, back into operational systems. This allows you to activate your data by making it available in the tools your business teams use every day like CRMs, marketing automation platforms, and advertising networks.

Let's take a look how to set up continuous queries for Cymbal Gaming's Galactic Grand Prix:

1
Write a standard SQL query.

2
Enable the job to run continuously.

3
Writes into BigQuery will be continuously pushed to Bigtable as they are received.

The image above shows writes continuously pushed to Bigtable, but the following are also supported destinations:

•
Pub/Sub: Publish continuous query output to Pub/Sub topics to trigger downstream applications.

•
Spanner: Export data to Spanner to serve data to application users without exhausting BigQuery quotas.

•
Other BigQuery tables: Write the results of a continuous query into another BigQuery table for further analysis.


## Bigtable: The solution for operational data

Throughout this course you've followed high-speed telemetry data from the Galactic Grand Prix. You built a pipeline that ingested that data with Pub/Sub, transformed it in real-time with Dataflow, and performed powerful, large-scale analytics with BigQuery. You just saw how to run complex, ad-hoc queries to analyze historical race performance, but what if you want to serve that data back to the game in real-time? How do you power the live leaderboards that fans are watching?

For these operational workloads that demand millions of lookups with single-digit millisecond latency we turn to the final main component of our streaming architecture: Bigtable, Google Cloud's fully managed, petabyte-scale NoSQL database service. This is where your data will land so that it's ready to be served instantly and at scale, driving the real-time applications that make the Galactic Grand Prix possible.

When working with streaming data on Google Cloud, a common question often arises: BigQuery or Bigtable. While both services can handle immense volumes of data, they are designed for different use cases and data access patterns. This section will help you understand when and how to use Bigtable, especially for application serving and real-time operations. This video provides a high-level overview of what each service is designed for, their core differences, and when to use them, specifically within the context of a fictional e-sports game.

### Bigtable features

The table below provides a clear comparison of both products and their features, further clarifying their distinct roles inside a streaming data pipeline.

Feature

Bigtable

BigQuery

Data structure

Unstructured. Optimized for NoSQL queries but supports GoogleSQL. It is a key-value and wide-column database designed for high throughput, low-latency workloads. It offers a flexible schema that adapts to evolving data needs.

Structured. Optimized for SQL queries but supports flexible, semi-structured data formats. It is a serverless data platform designed for petabyte-scale analytics. It supports a flexible schema using nested and repeated fields.

Deployment scope

Global. Ready for application serving. It supports topologies from a single zone up to eight regions. Multi-region, multi-primary deployments bring data closer to customers for best latencies.

Regional. Ready for analytics. Streaming performance in US, EU multi-regions is 500k rows per second per table (or 1M per second if deduplication is disabled in the US region).

Latency and throughput

Milliseconds for NoSQL queries over large datasets. It is a low-latency NoSQL database, ideal for latency-sensitive workloads like personalization, clickstream, IoT, and ML model training. It supports millions of reads per second (RPS) and predictable single-digit millisecond latency.

Seconds for interactive SQL queries over large datasets (petabytes). Offers near-real-time insights. Streaming insert performance is 500k rows per second per table in US/EU multi-regions (or 1M/sec with no dedupe in US region).

Primary use cases

Real-time application serving

IoT and time series data

Machine learning workloads (online - serving prediction)

Data integration

Real-time analytics

Data warehousing + business intelligence

Machine learning workloads (offline - training models and pre-processing)

Geospatial analysis

Scalability

Infinitely scalable with no limits on horizontal scalability. Decouples compute from storage. Automatically scales resources to adapt to server traffic and is optimized for high-throughput, low-latency application workloads. Its throughput scales linearly with the number of nodes, which can be adjusted to control Queries Per Second (QPS).

Serverless and designed for petabyte-scale analytics. Decouples compute from storage. Automatically manages and scales compute resources to meet query demands, allowing you to pay only for the storage you use and the queries you run, not for idle resources.

### Bigtable: The big picture

Beyond the comparison, let's consider the Galactic Grand Prix pipeline where Pub/Sub handles data movement, Dataflow provides real-time processing, and BigQuery manages massive amounts of analytics.

In this ecosystem, Bigtable provides the real-time serving layer for the analytics that BigQuery is storing. Bigtable acts as the low-latency analytics database, BigQuery serves as the enterprise data platform, and Looker visualizes insights by creating dashboards that query BigQuery - combining real-time streaming data from Bigtable with other business data

### Big table: Data schema

The reference architecture above establishes a complete data pipeline from raw event capture to interactive dashboards, enabling a deep dive into how a core component like Bigtable handles the data it receives. To ensure Bigtable handles that data efficiently, it is crucial to understand the principles of schema design.

### Schema design: Speed through simplicity

Bigtable's low-latency, high-throughput linear scalability is a direct result of its architecture. It prioritizes massive scale over strict ACID semantics, using a simple data model to optimize performance. Throughput, measured in Queries Per Second (QPS), scales proportionally with the number of nodes, making it ideal for internet-scale workloads with unpredictable traffic spikes.

### Moving data between BigQuery and Bigtable

A central challenge in moving your data between BigQuery and Bigtable is converting the structured data into Bigtable's wide column format. This quick demo highlights an effective row key design that combines columns from your BigQuery table that are most frequently used for lookups in WHERE clauses. This approach ensures that the row key, which serves as one of Bigtable's main indexes, is optimized to support your most common queries and access patterns.

### Common use cases

Having this overall data schema and flow in mind, and with a clear understanding of Bigtable features, let's take a look at some common Bigtable use cases.  

- Real-time application serving: In this use case, Bigtable acts as the operational application database. Insights from real-time data, processed by BigQuery continuous queries, are sent to Bigtable to be used for real-time application serving. This flow of data from the analytics platform back to the operational database showcases the reverse ETL pattern you learned about earlier.

- IOT and time-series data: Bigtable is a natural fit for both IoT and time-series data because it is optimized for ingesting, storing, and analyzing high volumes of data with low latency. In both of these use cases, Bigtable's performance is driven by its ability to retrieve data quickly based on a row-key. The key to success here is properly designed schema.

This image shows the row key - one of the most critical parts of designing a proper schema. The row key determines how data is distributed across Bigtable and therefore how efficiently your queries will run.

- Machine Learning: Bigtable is a natural fit for both IoT and time-series data because it is optimized for ingesting, storing, and analyzing high volumes of data with low latency. In both of these use cases, Bigtable's performance is driven by its ability to retrieve data quickly based on a row-key. The key to success here is properly designed schema.

This image shows the row key - one of the most critical parts of designing a proper schema. The row key determines how data is distributed across Bigtable and therefore how efficiently your queries will run.

- Adtech and Retail: For Adtech, Bigtable enables real-time bidding platforms to process millions of requests per second and perform instant, personalized ad targeting by quickly looking up user profiles and behavioral data. For Retail, it powers real-time product recommendation engines, manages massive inventory catalogs, and supports fraud detection by ingesting and analyzing customer activity streams at high speed, ensuring the customer experience remains fast and highly personalized.

### Moving data to and from Bigtable with Dataflow 

So you know why Bigtable is the ideal database for these demanding, real-time applications. The next logical question is, "How do I build the pipelines to get my data there?"

While you can always write a pipeline from scratch, Dataflow provides a much more streamlined and powerful approach using templates. These templates are designed to simplify data ingestion, making it easy to feed your Bigtable instances at scale. Dataflow has two templates for Bigtable that process data continuously: Bigtable change streams to BigQuery, and Bigtable change streams to Pub/Sub.

The following video introduces you to using templates in order to package Dataflow pipelines for deployment, making it easier to perform common data processing tasks between Bigtable and other services without needing to write or manage code.

### Monitoring and performance

While templates streamline common tasks, ensuring your pipelines perform optimally requires dedicated monitoring. Next, you'll learn about some key strategies for monitoring Bigtable and diagnosing performance issues.

### Troubleshooting 

If you think that Bigtable is acting as a performance bottleneck for your application, there are a variety of troubleshooting steps to try.

- Comment out the code that performs Bigtable reads and writes: If this resolves your performance issues, then you should examine how you are using Bigtable (schema design, connections to Cloud Bigtable, etc.). It's likely that your setup is less than optimal. If, on the other hand, the performance issue persists, the issue is probably not related to Bigtable.

- Make sure you create and reuse a single connection: Opening a connection to Bigtable is a relatively expensive operation. If you're using the Bigtable HBase Java client, make sure you're creating and sharing one long-lived Connection object across all of the threads in your application. The Connection object will automatically handle multiplexing across multiple simultaneous threads. Similarly, if you're using another client, make sure you're creating and reusing a single connection to Bigtable

- Make sure you read and write many different rows in your table: If reads and writes cannot be spread across all of your Bigtable nodes, performance will suffer. If you find that you're reading and writing only a small number of rows, you may need to redesign your schema to allow for a more even distribution of reads and writes.

- Verify that you see about the same performance for reads and writes: Certain issues can result in reads being much faster than writes, such as trying to read nonexistent row keys or large ranges of keys that contain only a small number of rows.

### From reactive troubleshooting to proactive workload isolation

While the previous sections focused on optimizing performance and troubleshooting existing workloads, a more strategic approach to prevent performance contention is to isolate analytical jobs from your primary application traffic, which can be done using Bigtable Data Boost.

With Bigtable Data Boost you:

Can run analytics and batch jobs on your production data without impacting the performance of your main cluster.

Use isolated, serverless compute resources that access your data directly from the underlying storage.

Eliminate the need for separate analytical databases and complex ETL pipelines, which simplifies your architecture and reduces costs.

Only pay for the compute resources you use.

